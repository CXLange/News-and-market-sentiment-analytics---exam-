{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "true_data = pd.read_csv('data/True.csv')\n",
    "fake_data = pd.read_csv('data/Fake.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a single, clean dataset with labels for true = 1 and fake = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 6 duplicate title rows from true articles\n",
      "Removed 184 duplicate title rows from fake articles\n",
      "\n",
      "Removed 229 duplicate text rows from true articles\n",
      "Removed 5398 duplicate text rows from fake articles\n",
      "______________________________________________________________________\n",
      "Removed 235 duplicate rows from true articles in total\n",
      "Removed 5582 duplicate rows from fake articles in total\n"
     ]
    }
   ],
   "source": [
    "# Creating a single dataset with labels for true = 1 and false = 0\n",
    "\n",
    "true_data['label'] = 1\n",
    "fake_data['label'] = 0\n",
    "\n",
    "# Cleaning true_data before concatinating\n",
    "## Removing journal identifier\n",
    "true_data['text'] = true_data['text'].str.partition('- ')[2]\n",
    "\n",
    "# Removing duplicates rows of titles where text is empty\n",
    "# Create a boolean placeholder to check if text is empty\n",
    "is_text_functionally_empty = (\n",
    "    true_data['text'].isna() | \n",
    "    true_data['text'].astype(str).str.strip().eq('')\n",
    ")\n",
    "# Drop rows if true and keep first instance of title if there are duplicates\n",
    "rows_to_drop = true_data[is_text_functionally_empty].duplicated(subset=['title'], keep='first')\n",
    "drop_indices = true_data[is_text_functionally_empty][rows_to_drop].index\n",
    "cleaned_true = true_data.drop(index=drop_indices)\n",
    "# Doing the same for fake articles\n",
    "fake_is_text_functionally_empty = (\n",
    "    fake_data['text'].isna() | \n",
    "    fake_data['text'].astype(str).str.strip().eq('')\n",
    ")\n",
    "rows_to_drop = fake_data[fake_is_text_functionally_empty].duplicated(subset=['title'], keep='first')\n",
    "drop_indices = fake_data[fake_is_text_functionally_empty][rows_to_drop].index\n",
    "cleaned_fake = fake_data.drop(index=drop_indices)\n",
    "\n",
    "print(f\"Removed {len(true_data)-len(cleaned_true)} duplicate title rows from true articles\")\n",
    "print(f\"Removed {len(fake_data)-len(cleaned_fake)} duplicate title rows from fake articles\")\n",
    "print(\"\")\n",
    "\n",
    "# Removing duplicates of text but keeping unique rows of title\n",
    "has_content = ~(\n",
    "    cleaned_fake['text'].isna() |\n",
    "    cleaned_fake['text'].astype(str).str.strip().eq('')\n",
    ")\n",
    "rows_to_remove = cleaned_fake[has_content].duplicated(subset=['text'], keep='first')\n",
    "\n",
    "drop_indices = cleaned_fake[has_content][rows_to_remove].index\n",
    "\n",
    "new_cleaned_fake = cleaned_fake.drop(index=drop_indices)\n",
    "\n",
    "# Doing the same for true articles\n",
    "\n",
    "has_content = ~(\n",
    "    cleaned_true['text'].isna() |\n",
    "    cleaned_true['text'].astype(str).str.strip().eq('')\n",
    ")\n",
    "rows_to_remove = cleaned_true[has_content].duplicated(subset=['text'], keep='first')\n",
    "\n",
    "drop_indices = cleaned_true[has_content][rows_to_remove].index\n",
    "\n",
    "new_cleaned_true = cleaned_true.drop(index=drop_indices)\n",
    "\n",
    "print(f\"Removed {len(cleaned_true)-len(new_cleaned_true)} duplicate text rows from true articles\")\n",
    "print(f\"Removed {len(cleaned_fake)-len(new_cleaned_fake)} duplicate text rows from fake articles\")\n",
    "print(\"______________________________________________________________________\")\n",
    "print(f\"Removed {len(true_data)-len(new_cleaned_true)} duplicate rows from true articles in total\")\n",
    "print(f\"Removed {len(fake_data)-len(new_cleaned_fake)} duplicate rows from fake articles in total\")\n",
    "\n",
    "# Concatinating dataframes\n",
    "data = pd.concat([new_cleaned_true, new_cleaned_fake])\n",
    "data = data.reset_index(drop=True, inplace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text standardizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text standardizing\n",
    "import string\n",
    "\n",
    "# Creating new columns to preserve original text\n",
    "data['title_standard'] = data['title']\n",
    "data['text_standard'] = data['text']\n",
    "\n",
    "# Removing punctuations including special letter which didn't get picked up by string.punctuation\n",
    "punctuation_and_special = string.punctuation + '“”‘’' \n",
    "punctuation = str.maketrans('', '', punctuation_and_special)\n",
    "\n",
    "data['title_standard'] = data['title'].astype(str).str.translate(punctuation)\n",
    "data['text_standard'] = data['text'].astype(str).str.translate(punctuation)\n",
    "\n",
    "# Lowercasing \n",
    "data['title_standard'] = data['title_standard'].astype(str).str.strip().str.lower()\n",
    "data['text_standard'] = data['text_standard'].astype(str).str.strip().str.lower()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aiming for a baseline with naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Naive Bayes Classification Results (Using Raw Word Counts) ---\n",
      "Accuracy: 0.9424\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.91      0.94      3580\n",
      "           1       0.93      0.97      0.95      4237\n",
      "\n",
      "    accuracy                           0.94      7817\n",
      "   macro avg       0.94      0.94      0.94      7817\n",
      "weighted avg       0.94      0.94      0.94      7817\n",
      "\n",
      "--- Top 20 Words Predicting FAKE News (Label 0) ---\n",
      "| feature             |   fake_score |\n",
      "|:--------------------|-------------:|\n",
      "| 2017realdonaldtrump |      7.0647  |\n",
      "| 21wire              |      6.35848 |\n",
      "| belowfeatured       |      6.28281 |\n",
      "| getty               |      6.2585  |\n",
      "| 2017the             |      6.08819 |\n",
      "| flickr              |      6.06944 |\n",
      "| 21wiretv            |      5.91799 |\n",
      "| 2016realdonaldtrump |      5.82926 |\n",
      "| somodevillagetty    |      5.74693 |\n",
      "| screenshot          |      5.74319 |\n",
      "| acr                 |      5.70112 |\n",
      "| cdata               |      5.68933 |\n",
      "| js                  |      5.66127 |\n",
      "| filessupport        |      5.63658 |\n",
      "| 2017trump           |      5.53122 |\n",
      "| wonggetty           |      5.47895 |\n",
      "| 2016the             |      5.45426 |\n",
      "| reilly              |      5.38169 |\n",
      "| finicum             |      5.35995 |\n",
      "| angerergetty        |      5.35444 |\n",
      "\n",
      "--- Top 20 Words Predicting TRUE News (Label 1) ---\n",
      "| feature    |   fake_score |\n",
      "|:-----------|-------------:|\n",
      "| rohingya   |     -7.25682 |\n",
      "| myanmar    |     -6.45431 |\n",
      "| rakhine    |     -6.43847 |\n",
      "| puigdemont |     -6.0706  |\n",
      "| partys     |     -5.9148  |\n",
      "| fdp        |     -5.88906 |\n",
      "| zuma       |     -5.7973  |\n",
      "| suu        |     -5.76295 |\n",
      "| kyi        |     -5.75486 |\n",
      "| hariri     |     -5.66858 |\n",
      "| rajoy      |     -5.61238 |\n",
      "| mnangagwa  |     -5.55449 |\n",
      "| odinga     |     -5.5445  |\n",
      "| anc        |     -5.53103 |\n",
      "| catalan    |     -5.47407 |\n",
      "| chinas     |     -5.42001 |\n",
      "| juncker    |     -5.40474 |\n",
      "| countrys   |     -5.30789 |\n",
      "| barnier    |     -5.30364 |\n",
      "| kuczynski  |     -5.28214 |\n"
     ]
    }
   ],
   "source": [
    "# Baseline naive bayes model for article texts\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "\n",
    "# Define Features (X) and Target (y)\n",
    "X = data['text_standard']  \n",
    "y = data['label']\n",
    "\n",
    "# Splitting the data and making sure we get the same number of labels in each set with stratify=y\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Setting max_df to 0.7 we exclude words that appears in more than 70% of all articles in the training set so we should get more unique words\n",
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_counts = count_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data\n",
    "X_test_counts = count_vectorizer.transform(X_test)\n",
    "\n",
    "# Train Multinomial Naive Bayes Classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(X_train_counts, y_train)\n",
    "\n",
    "# Predict and Evaluate\n",
    "y_pred_counts = nb_classifier.predict(X_test_counts)\n",
    "\n",
    "# Get the feature names to see which words are helping us predict\n",
    "feature_names = count_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Getting log coefficients \n",
    "log_probs_fake = nb_classifier.feature_log_prob_[0]\n",
    "log_probs_true = nb_classifier.feature_log_prob_[1]\n",
    "\n",
    "# Create a DataFrame so we can sort them \n",
    "feature_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'log_prob_fake': log_probs_fake,\n",
    "    'log_prob_true': log_probs_true\n",
    "})\n",
    "\n",
    "# Calculate the difference between coefficients \n",
    "# A larger positive difference means the word is highly associated with fake news\n",
    "feature_df['fake_score'] = feature_df['log_prob_fake'] - feature_df['log_prob_true']\n",
    "\n",
    "# Get top 20 for Fake (highest positive scores)\n",
    "top_fake_features = feature_df.sort_values(by='fake_score', ascending=False).head(20)\n",
    "\n",
    "# Get top 20 for True (lowest negative scores)\n",
    "top_true_features = feature_df.sort_values(by='fake_score', ascending=True).head(20)\n",
    "\n",
    "\n",
    "# Print results\n",
    "print(\"--- Naive Bayes Classification Results\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_counts):.4f}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_counts))\n",
    "\n",
    "# Print important classifying words\n",
    "print(\"--- Top 20 Words Predicting FAKE News (Label 0) ---\")\n",
    "print(top_fake_features[['feature', 'fake_score']].to_markdown(index=False))\n",
    "\n",
    "print(\"\\n--- Top 20 Words Predicting TRUE News (Label 1) ---\")\n",
    "print(top_true_features[['feature', 'fake_score']].to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do the same for article titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Naive Bayes Classification Results (Using Raw Word Counts) ---\n",
      "Accuracy: 0.9364\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.94      0.93      3580\n",
      "           1       0.95      0.93      0.94      4237\n",
      "\n",
      "    accuracy                           0.94      7817\n",
      "   macro avg       0.94      0.94      0.94      7817\n",
      "weighted avg       0.94      0.94      0.94      7817\n",
      "\n",
      "--- Top 20 Words Predicting FAKE News (Label 0) ---\n",
      "| feature     |   fake_score |\n",
      "|:------------|-------------:|\n",
      "| hillarys    |      5.25011 |\n",
      "| wow         |      5.09675 |\n",
      "| video       |      5.09181 |\n",
      "| heres       |      5.09121 |\n",
      "| hilarious   |      4.89547 |\n",
      "| busted      |      4.62564 |\n",
      "| bombshell   |      4.51338 |\n",
      "| hilariously |      4.49337 |\n",
      "| epic        |      4.47297 |\n",
      "| sarah       |      4.45213 |\n",
      "| gop         |      4.44685 |\n",
      "| supporter   |      4.43085 |\n",
      "| fck         |      4.32907 |\n",
      "| brilliant   |      4.32907 |\n",
      "| awesome     |      4.30497 |\n",
      "| dem         |      4.30497 |\n",
      "| lol         |      4.30497 |\n",
      "| disgusting  |      4.24206 |\n",
      "| racist      |      4.24206 |\n",
      "| boiler      |      4.21574 |\n",
      "\n",
      "--- Top 20 Words Predicting TRUE News (Label 1) ---\n",
      "| feature    |   fake_score |\n",
      "|:-----------|-------------:|\n",
      "| factbox    |     -5.88249 |\n",
      "| myanmar    |     -5.38495 |\n",
      "| catalan    |     -5.17065 |\n",
      "| rohingya   |     -5.13218 |\n",
      "| kurdish    |     -4.77457 |\n",
      "| envoy      |     -4.73647 |\n",
      "| xi         |     -4.6126  |\n",
      "| referendum |     -4.47119 |\n",
      "| catalonia  |     -4.45845 |\n",
      "| ria        |     -4.40581 |\n",
      "| zimbabwe   |     -4.36443 |\n",
      "| hariri     |     -4.27613 |\n",
      "| zimbabwes  |     -4.22888 |\n",
      "| asia       |     -4.22888 |\n",
      "| bangladesh |     -4.21262 |\n",
      "| ireland    |     -4.21262 |\n",
      "| kurds      |     -4.16219 |\n",
      "| chinas     |     -4.11813 |\n",
      "| mugabe     |     -4.10908 |\n",
      "| uks        |     -4.09073 |\n"
     ]
    }
   ],
   "source": [
    "# We do the same for the article titles\n",
    "\n",
    "X = data['title_standard']\n",
    "y = data['label']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "\n",
    "X_train_counts = count_vectorizer.fit_transform(X_train)\n",
    "\n",
    "\n",
    "X_test_counts = count_vectorizer.transform(X_test)\n",
    "\n",
    "\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(X_train_counts, y_train)\n",
    "\n",
    "\n",
    "y_pred_counts = nb_classifier.predict(X_test_counts)\n",
    "\n",
    "feature_names = count_vectorizer.get_feature_names_out()\n",
    "\n",
    "log_probs_fake = nb_classifier.feature_log_prob_[0]\n",
    "log_probs_true = nb_classifier.feature_log_prob_[1]\n",
    "\n",
    "\n",
    "feature_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'log_prob_fake': log_probs_fake,\n",
    "    'log_prob_true': log_probs_true\n",
    "})\n",
    "\n",
    "feature_df['fake_score'] = feature_df['log_prob_fake'] - feature_df['log_prob_true']\n",
    "top_fake_features = feature_df.sort_values(by='fake_score', ascending=False).head(20)\n",
    "top_true_features = feature_df.sort_values(by='fake_score', ascending=True).head(20)\n",
    "\n",
    "print(\"--- Naive Bayes Classification Results (Using Raw Word Counts) ---\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_counts):.4f}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_counts))\n",
    "print(\"--- Top 20 Words Predicting FAKE News (Label 0) ---\")\n",
    "print(top_fake_features[['feature', 'fake_score']].to_markdown(index=False))\n",
    "print(\"\\n--- Top 20 Words Predicting TRUE News (Label 1) ---\")\n",
    "print(top_true_features[['feature', 'fake_score']].to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Added cleaning step to handle noise observed from naive bayes model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Added cleaning step to handle noise in fake articles\n",
    "data['text_cleaned'] = data['text_standard']\n",
    "# Remove common URL patterns and link shorteners\n",
    "data['text_cleaned'] = data['text_cleaned'].str.replace(r'http[s]?://\\S+|www\\.\\S+|\\S+\\.(com|org|net|co|ly)|pictwittercom|httpstco|bitly', '', regex=True)\n",
    "# Remove photo/site credit words (getty, flickr, wikimedia, etc.)\n",
    "credit_patterns = r'getty|flickr|wikimedia|belowfeatured|somodevillagetty|mcnameegetty|angerergetty|wiretv|acr|cdata|filessupport'\n",
    "data['text_cleaned'] = data['text_cleaned'].str.replace(credit_patterns, '', regex=True)\n",
    "# Remove common code snippets and internal tags\n",
    "code_patterns = r'var|js|dgetelementsbytagnames|dcreateelements|dgetelementbyidid|jssrc|jsid|wfb|featured|screenshot|raedle|gage|donnell|whinedr|src|xfbml|parentnodeinsertbefore|versionv|screengrab|subscribing|nyp'\n",
    "data['text_cleaned'] = data['text_cleaned'].str.replace(code_patterns, ' ', regex=True)\n",
    "# Find all words that consist only of letters (a-z) and more than 2 characters long to get rid of fx 21Wire\n",
    "data['text_cleaned'] = data['text_cleaned'].str.findall(r'[a-z]{2,}')\n",
    "# Join the tokenized words into single string again\n",
    "data['text_cleaned'] = data['text_cleaned'].str.join(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-run of bayes on newly cleaned article text since text had higher accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Naive Bayes Classification Results (Using Raw Word Counts) ---\n",
      "Accuracy: 0.9418\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.91      0.94      3580\n",
      "           1       0.93      0.96      0.95      4237\n",
      "\n",
      "    accuracy                           0.94      7817\n",
      "   macro avg       0.94      0.94      0.94      7817\n",
      "weighted avg       0.94      0.94      0.94      7817\n",
      "\n",
      "--- Top 20 Words Predicting FAKE News (Label 0) ---\n",
      "| feature        |   fake_score |\n",
      "|:---------------|-------------:|\n",
      "| reilly         |      5.38192 |\n",
      "| finicum        |      5.36565 |\n",
      "| fcking         |      5.23757 |\n",
      "| henningsen     |      5.18661 |\n",
      "| whined         |      5.09064 |\n",
      "| bundy          |      5.05214 |\n",
      "| hammonds       |      4.81013 |\n",
      "| behar          |      4.81013 |\n",
      "| fck            |      4.7909  |\n",
      "| shit           |      4.7844  |\n",
      "| somodevilla    |      4.77129 |\n",
      "| watters        |      4.75129 |\n",
      "| elizabethforma |      4.74114 |\n",
      "| cher           |      4.72052 |\n",
      "| hilariously    |      4.71005 |\n",
      "| sarahpalinusa  |      4.69946 |\n",
      "| tantrum        |      4.69413 |\n",
      "| itthe          |      4.68877 |\n",
      "| hissy          |      4.68877 |\n",
      "| gitmo          |      4.67796 |\n",
      "\n",
      "--- Top 20 Words Predicting TRUE News (Label 1) ---\n",
      "| feature    |   fake_score |\n",
      "|:-----------|-------------:|\n",
      "| rohingya   |     -7.2566  |\n",
      "| myanmar    |     -6.45408 |\n",
      "| rakhine    |     -6.43824 |\n",
      "| puigdemont |     -6.07037 |\n",
      "| partys     |     -5.91457 |\n",
      "| zuma       |     -5.79707 |\n",
      "| suu        |     -5.76273 |\n",
      "| kyi        |     -5.75463 |\n",
      "| hariri     |     -5.66836 |\n",
      "| rajoy      |     -5.61215 |\n",
      "| mnangagwa  |     -5.55426 |\n",
      "| odinga     |     -5.54427 |\n",
      "| anc        |     -5.53081 |\n",
      "| catalan    |     -5.47385 |\n",
      "| tmsnrtrs   |     -5.47145 |\n",
      "| chinas     |     -5.41978 |\n",
      "| juncker    |     -5.40451 |\n",
      "| countrys   |     -5.30766 |\n",
      "| barnier    |     -5.30342 |\n",
      "| kuczynski  |     -5.28191 |\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define Features (X) and Target (y)\n",
    "X = data['text_cleaned']  \n",
    "y = data['label']\n",
    "\n",
    "# Splitting the data and making sure we get the same number of labels in each set with stratify=y\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Setting max_df to 0.7 we exclude words that appears in more than 70% of all articles in the training set so we should get more unique words\n",
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_counts = count_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data\n",
    "X_test_counts = count_vectorizer.transform(X_test)\n",
    "\n",
    "# Train Multinomial Naive Bayes Classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(X_train_counts, y_train)\n",
    "\n",
    "# 5. Predict and Evaluate\n",
    "y_pred_counts = nb_classifier.predict(X_test_counts)\n",
    "\n",
    "# Get the feature names to see which words are helping us predict\n",
    "feature_names = count_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Getting log coefficients \n",
    "log_probs_fake = nb_classifier.feature_log_prob_[0]\n",
    "log_probs_true = nb_classifier.feature_log_prob_[1]\n",
    "\n",
    "# Create a DataFrame so we can sort them \n",
    "feature_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'log_prob_fake': log_probs_fake,\n",
    "    'log_prob_true': log_probs_true\n",
    "})\n",
    "\n",
    "# Calculate the difference between coefficients \n",
    "# A larger positive difference means the word is highly associated with fake news\n",
    "feature_df['fake_score'] = feature_df['log_prob_fake'] - feature_df['log_prob_true']\n",
    "\n",
    "# 5. Get top 20 for Fake (highest positive scores)\n",
    "top_fake_features = feature_df.sort_values(by='fake_score', ascending=False).head(20)\n",
    "\n",
    "# 6. Get top 20 for True (lowest negative scores)\n",
    "top_true_features = feature_df.sort_values(by='fake_score', ascending=True).head(20)\n",
    "\n",
    "\n",
    "# Print results\n",
    "print(\"--- Naive Bayes Classification Results (Using Raw Word Counts) ---\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_counts):.4f}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_counts))\n",
    "\n",
    "# Print important classifying words\n",
    "print(\"--- Top 20 Words Predicting FAKE News (Label 0) ---\")\n",
    "print(top_fake_features[['feature', 'fake_score']].to_markdown(index=False))\n",
    "\n",
    "print(\"\\n--- Top 20 Words Predicting TRUE News (Label 1) ---\")\n",
    "print(top_true_features[['feature', 'fake_score']].to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying with cosine and k-means clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization of text sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "from collections.abc import Iterable\n",
    "\n",
    "# Cleaning process to split on \".\" before removing other punctuation\n",
    "# Initial Split (We use the original 'text' column)\n",
    "data['sentence_list'] = data['text'].str.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?|\\!)\\s', expand=False)\n",
    "\n",
    "\n",
    "def clean_and_filter_sentences(sentence_list):\n",
    "    \"\"\"Strips whitespace and filters out empty/short strings.\"\"\"\n",
    "    try:\n",
    "        if pd.isna(sentence_list).all():\n",
    "            return []\n",
    "    except AttributeError:\n",
    "        # handle nans\n",
    "        if pd.isna(sentence_list):\n",
    "            return []\n",
    "    \n",
    "    # array-like objects into a Python list\n",
    "    if isinstance(sentence_list, (np.ndarray, pd.Series)):\n",
    "        sentence_list = sentence_list.tolist()\n",
    "        \n",
    "    # Handle non-iterable inputs\n",
    "    if not isinstance(sentence_list, Iterable) or isinstance(sentence_list, str):\n",
    "        # Wrap into a list\n",
    "        sentence_list = [str(sentence_list)]\n",
    "    # Strip leading/trailing whitespace after \".\" from sentences\n",
    "    cleaned_list = [s.strip() for s in sentence_list]\n",
    "    \n",
    "    # Filter out elements that are empty or too short (like just \"U. \")\n",
    "    final_sentences = [s for s in cleaned_list if len(s) > 5]\n",
    "    \n",
    "    return final_sentences\n",
    "\n",
    "# Apply the cleaning and filtering\n",
    "data['sentences'] = data['sentence_list'].apply(clean_and_filter_sentences)\n",
    "\n",
    "\n",
    "\n",
    "# Clean sentences like we did before with the entire article text now just on our sentence tokens\n",
    "# Define all cleaning patterns\n",
    "punctuation_to_remove = string.punctuation + '“”‘’' \n",
    "credit_patterns = r'getty|flickr|wikimedia|belowfeatured|somodevillagetty|mcnameegetty|angerergetty|wiretv|acr|cdata|filessupport'\n",
    "code_patterns = r'var|js|dgetelementsbytagnames|dcreateelements|dgetelementbyidid|jssrc|jsid|wfb|featured|screenshot|raedle|gage|donnell|whinedr|src|xfbml|parentnodeinsertbefore|versionv|screengrab|subscribing|nyp'\n",
    "url_patterns = r'http[s]?://\\S+|www\\.\\S+|\\S+\\.(com|org|net|co|ly)|pictwittercom|httpstco|bitly'\n",
    "\n",
    "def apply_text_cleaning(sentence_list):\n",
    "    \"\"\"Applies all cleaning rules to every string inside the list.\"\"\"\n",
    "    cleaned_sentences = []\n",
    "    \n",
    "    for sentence in sentence_list:\n",
    "        text = str(sentence)\n",
    "        \n",
    "        # Lowercasing and strip\n",
    "        text = text.strip().lower()\n",
    "\n",
    "        # Remove URLs and links\n",
    "        text = re.sub(url_patterns, '', text)\n",
    "        \n",
    "        # Remove credit/code patterns\n",
    "        text = re.sub(credit_patterns, '', text)\n",
    "        text = re.sub(code_patterns, ' ', text)\n",
    "        \n",
    "        # Remove punctuation\n",
    "        text = text.translate(str.maketrans('', '', punctuation_to_remove))\n",
    "        \n",
    "        cleaned_sentences.append(text)\n",
    "        \n",
    "    return cleaned_sentences\n",
    "\n",
    "# Apply the complex cleaning function to get our clean sentence tokens\n",
    "data['text_sentence_tokens'] = data['sentences'].apply(apply_text_cleaning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embed sentence tokens\n",
    "\n",
    "#### Note, embedding the words of the sentences takes very long and requires too much memory making the kernel crash. So we stick with embedding the sentences of the articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATING EMBEDDINGS FOR TITLES\n",
    "import pandas as pd\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Initialize the encoder\n",
    "title_sent_encoder = SentenceTransformer('all-MiniLM-L12-v2')\n",
    "\n",
    "# Extract columns and convert to lists (using existing column names)\n",
    "title_list = data['title_standard'].tolist()\n",
    "\n",
    "# Generate embeddings for titles which consist of one sentence \n",
    "title_embeddings = title_sent_encoder.encode(title_list, show_progress_bar=True)\n",
    "\n",
    "# Correctly store the 2D arrays back into the DataFrame\n",
    "data['title_embedding'] = pd.Series(list(title_embeddings), index=data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "# Flatten the list of lists in the 'text_sentence_tokens' column \n",
    "# into one single list of all sentences.\n",
    "all_sentences = list(chain.from_iterable(data['text_sentence_tokens'].dropna()))\n",
    "\n",
    "print(f\"Total number of individual sentences to embed: {len(all_sentences)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This part takes a while\n",
    "\n",
    "# Initialize the Sentence Transformer model\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Initialize the Sentence Transformer model\n",
    "sent_encoder = SentenceTransformer('all-MiniLM-L12-v2')\n",
    "\n",
    "# Run the encoding on the flattened list\n",
    "print(\"Starting embedding process...\")\n",
    "X_embeddings = sent_encoder.encode(\n",
    "    all_sentences, \n",
    "    show_progress_bar=True,\n",
    "    convert_to_tensor=False \n",
    ")\n",
    "\n",
    "print(f\"Embedding complete. Final embedding shape: {X_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# The dimension of our embedding vectors 384 for all-miniLM transformer\n",
    "embedding_dimension = X_embeddings.shape[1] \n",
    "\n",
    "mean_embeddings = []\n",
    "current_index = 0\n",
    "\n",
    "# Iterate through each row in our DataFrame\n",
    "for index, sentence_list in data['text_sentence_tokens'].items():\n",
    "    num_sentences = len(sentence_list)\n",
    "    \n",
    "    if num_sentences == 0:\n",
    "        # Handle rows where cleaning resulted in zero valid sentences\n",
    "        article_embedding = np.zeros(embedding_dimension)\n",
    "    else:\n",
    "        # Extract the sentences belonging to the current article\n",
    "        article_sentence_embeddings = X_embeddings[current_index : current_index + num_sentences]\n",
    "        \n",
    "        # Calculate the average vector across all sentences (axis=0 averages down the rows)\n",
    "        article_embedding = np.mean(article_sentence_embeddings, axis=0)\n",
    "        \n",
    "        # Move the pointer to the start of the next article embedding\n",
    "        current_index += num_sentences\n",
    "        \n",
    "    mean_embeddings.append(article_embedding)\n",
    "\n",
    "# The total number of new embeddings should match the number of rows in our DataFrame\n",
    "print(f\"Total number of aggregated article embeddings created: {len(mean_embeddings)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying with cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#-------- Uncomment one of the text_features to choose between the title sentence embeddings or the text embeddings------------\n",
    "\n",
    "# ------ Title embeddings ------------\n",
    "#text_features = np.stack(data['title_embedding'].to_numpy())\n",
    "# ------------------------------------\n",
    "\n",
    "# ------ text embeddings  ------------\n",
    "text_features = np.stack(data['aggregated_text_embedding'].to_numpy())\n",
    "# ------------------------------------\n",
    "\n",
    "X = text_features\n",
    "y = data['label'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame({'embeddings': list(X_train), 'label': y_train})\n",
    "\n",
    "title_mean = df_train.groupby('label')['embeddings'].apply(\n",
    "    lambda x: np.mean(np.stack(x.values), axis=0)\n",
    ")\n",
    "\n",
    "title_mean_true = title_mean[1]\n",
    "title_mean_fake = title_mean[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "\n",
    "mean_matrix = np.stack([title_mean_fake, title_mean_true])\n",
    "\n",
    "\n",
    "distance_matrix = cosine_distances(X_test, mean_matrix)\n",
    "\n",
    "\n",
    "y_pred = np.argmin(distance_matrix, axis=1)\n",
    "\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred, target_names=['Fake (0)', 'True (1)'])\n",
    "\n",
    "print(f\"Accuracy on unseen Test Set: {accuracy:.4f}\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "import numpy as np \n",
    "\n",
    "\n",
    "title_features = data['title_embedding']\n",
    "\n",
    "# I use umap, since t-sne was problematic for me, to reduce embedding dimensions to the 3 most important or the ones that explain the most variance.\n",
    "reducer = umap.UMAP(n_components=3, random_state=20)\n",
    "\n",
    "# Convert the series of embedding arrays into a 2D numpy array\n",
    "X_features = np.stack(title_features.to_numpy())\n",
    "\n",
    "X_umap = reducer.fit_transform(X_features)\n",
    "\n",
    "print(\"UMAP transformation complete.\")\n",
    "print(f\"UMAP Output Shape: {X_umap.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "silhouette_values = []\n",
    "\n",
    "for k in range(2, 10):\n",
    "    print(f'fitting k = {k}')\n",
    "    kmeans = KMeans(n_clusters=k, random_state=20, n_init=10)\n",
    "    cluster_labels = kmeans.fit_predict(X_umap)\n",
    "    \n",
    "    # Calculate silhouette score\n",
    "    score = silhouette_score(X_umap, cluster_labels)\n",
    "    silhouette_values.append(score)\n",
    "\n",
    "# plotting\n",
    "plt.plot(range(2, 10), silhouette_values, marker='x')\n",
    "plt.title('Silhouette Score vs Number of Clusters')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "elbow_point = 5\n",
    "\n",
    "# Refit with optimal clusters\n",
    "kmeans = KMeans(n_clusters=elbow_point, random_state=20, n_init = 10)\n",
    "kmeans.fit(X_umap) \n",
    "\n",
    "# Re-assign cluster labels based on the optimal clustering\n",
    "cluster_assignments = kmeans.labels_\n",
    "\n",
    "paragraphs = data['title'].tolist() \n",
    "\n",
    "central_sentences = []\n",
    "\n",
    "for cluster_id in range(elbow_point): \n",
    " cluster_indices = (cluster_assignments == cluster_id)\n",
    " cluster_X_paragraphs = np.array(paragraphs)[cluster_indices]\n",
    " X_umap_cluster = X_umap[cluster_indices]\n",
    "\n",
    " if len(cluster_X_paragraphs) > 0:\n",
    "\n",
    "  cluster_umap_points = X_umap[cluster_indices]\n",
    "  if len(cluster_umap_points) > 0:\n",
    "      centroid_umap = kmeans.cluster_centers_[cluster_id]\n",
    "\n",
    "      distances_to_centroid = np.sum((cluster_umap_points - centroid_umap)**2, axis=1)\n",
    "      closest_point_index_in_cluster_umap = np.argmin(distances_to_centroid)\n",
    "\n",
    "\n",
    "      central_sentence = cluster_X_paragraphs[closest_point_index_in_cluster_umap]\n",
    "      central_sentences.append(central_sentence)\n",
    "\n",
    "# Visualize clusters in 2D using X_umap\n",
    "scatter = plt.scatter(X_umap[:, 0], X_umap[:, 1], c=cluster_assignments, cmap='viridis', alpha=0.5)\n",
    "plt.legend(*scatter.legend_elements(), title=\"Clusters\")\n",
    "plt.title(f'Clusters with sentence-level transformer embeddings {elbow_point}')\n",
    "plt.show()\n",
    "\n",
    "# Print central sentences of clusters\n",
    "print(\"\\nCentral Sentences:\")\n",
    "for i, sentence in enumerate(central_sentences):\n",
    " print(f\"Cluster {i}: {sentence}\\n{'-' * 50}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We add cluster assignments to our df\n",
    "data['cluster_id'] = cluster_assignments\n",
    "\n",
    "# Group by cluster_id and label to see the distribution of true/fake articles in each cluster\n",
    "cluster_label_distribution = data.groupby(['cluster_id', 'label']).size().unstack(fill_value=0)\n",
    "\n",
    "cluster_label_distribution.rename(columns={0: 'Fake Articles', 1: 'True Articles'}, inplace=True)\n",
    "\n",
    "display(cluster_label_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "topic_classifier = pipeline(\"text-classification\", model=\"classla/multilingual-IPTC-news-topic-classifier\", device=0, max_length=512, truncation=True)\n",
    "\n",
    "print(f\"Applying topic classifier to {len(data['text_cleaned'])} articles. This might take a while...\")\n",
    "data['topic_predictions'] = data['text_cleaned'].progress_apply(lambda x: topic_classifier(x)[0] if pd.notna(x) and x != '' else None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subsetting dataframe with overlapping clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_df = data[data['cluster_id'].isin([1, 2])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 3 topics in subset\n",
    "for label in subset_df['label'].unique():\n",
    "    class_subset = subset_df[subset_df['label'] == label]\n",
    "    \n",
    "\n",
    "    topic_series = class_subset['topic_predictions'].apply(lambda x: x['label'] if isinstance(x, dict) and 'label' in x else None)\n",
    "    \n",
    "    top_3_topics = topic_series.value_counts().head(3)\n",
    "    \n",
    "    label_name = \"True\" if label == 1 else \"Fake\"\n",
    "    print(f\"Top 3 topics for {label_name} (label {label}):\")\n",
    "    print(top_3_topics)\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Subjectivity model\n",
    "classify = pipeline(\n",
    "    task=\"text-classification\",\n",
    "    model=\"cffl/bert-base-styleclassification-subjective-neutral\",\n",
    "    return_all_scores=True,\n",
    ")\n",
    "\n",
    "results = classify(subset_df['title_standard'].tolist(), truncation=True)\n",
    "\n",
    "def get_top_style(scores):\n",
    "    return max(scores, key=lambda x: x['score'])['label']\n",
    "\n",
    "subset_df['style'] = [get_top_style(res) for res in results]\n",
    "\n",
    "for label in [0, 1]:\n",
    "    label_name = \"True\" if label == 1 else \"Fake\"\n",
    "    print(f\"Style/Subjectivity for {label_name} (label {label}):\")\n",
    "    print(subset_df[subset_df['label'] == label]['style'].value_counts())\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALL ARTICLES TONE\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# Model\n",
    "classify = pipeline(\n",
    "    task=\"text-classification\",\n",
    "    model=\"cffl/bert-base-styleclassification-subjective-neutral\",\n",
    "    return_all_scores=True,\n",
    ")\n",
    "\n",
    "results = classify(data['title_standard'].tolist(), truncation=True)\n",
    "\n",
    "def get_top_style(scores):\n",
    "    return max(scores, key=lambda x: x['score'])['label']\n",
    "\n",
    "data['style'] = [get_top_style(res) for res in results]\n",
    "\n",
    "\n",
    "for label in [0, 1]:\n",
    "    label_name = \"True\" if label == 1 else \"Fake\"\n",
    "    print(f\"Style/Subjectivity for {label_name} (label {label}):\")\n",
    "    print(data[data['label'] == label]['style'].value_counts())\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EMOTIONS\n",
    "from transformers import pipeline\n",
    "classifier = pipeline(\"text-classification\", model=\"j-hartmann/emotion-english-distilroberta-base\", top_k=2)\n",
    "\n",
    "results = classifier(data['title_standard'].tolist(), truncation=True)\n",
    "\n",
    "def get_top_style(scores):\n",
    "    return max(scores, key=lambda x: x['score'])['label']\n",
    "\n",
    "data['emotions'] = [get_top_style(res) for res in results]\n",
    "\n",
    "for label in [0, 1]:\n",
    "    label_name = \"True\" if label == 1 else \"Fake\"\n",
    "    print(f\"Emotions for {label_name} (label {label}):\")\n",
    "    print(data[data['label'] == label]['emotions'].value_counts())\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Use clusters since these are made based on embeddings.\n",
    "features_embeddings = pd.DataFrame(data['cluster_id']).reset_index(drop=True)\n",
    "\n",
    "\n",
    "features_meta = pd.get_dummies(data[['style', 'emotions']]).reset_index(drop=True)\n",
    "\n",
    "# Combine\n",
    "X = pd.concat([features_embeddings, features_meta], axis=1)\n",
    "y = data['label']\n",
    "\n",
    "X.columns = X.columns.astype(str)\n",
    "# Split to train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Start tree\n",
    "tree_clf = DecisionTreeClassifier(max_depth=4, random_state=42)\n",
    "tree_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = tree_clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "\n",
    "from sklearn.tree import plot_tree\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(25,12))\n",
    "plot_tree(tree_clf, \n",
    "          feature_names=X.columns, \n",
    "          class_names=['Fake', 'True'], \n",
    "          filled=True, \n",
    "          impurity=False, \n",
    "          proportion=True, \n",
    "          precision=2\n",
    "          )\n",
    " \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
