{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "true_data = pd.read_csv('data/True.csv')\n",
    "fake_data = pd.read_csv('data/Fake.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True articles\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>As U.S. budget fight looms, Republicans flip t...</td>\n",
       "      <td>WASHINGTON (Reuters) - The head of a conservat...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 31, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U.S. military to accept transgender recruits o...</td>\n",
       "      <td>WASHINGTON (Reuters) - Transgender people will...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 29, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Senior U.S. Republican senator: 'Let Mr. Muell...</td>\n",
       "      <td>WASHINGTON (Reuters) - The special counsel inv...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 31, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FBI Russia probe helped by Australian diplomat...</td>\n",
       "      <td>WASHINGTON (Reuters) - Trump campaign adviser ...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 30, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Trump wants Postal Service to charge 'much mor...</td>\n",
       "      <td>SEATTLE/WASHINGTON (Reuters) - President Donal...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 29, 2017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  As U.S. budget fight looms, Republicans flip t...   \n",
       "1  U.S. military to accept transgender recruits o...   \n",
       "2  Senior U.S. Republican senator: 'Let Mr. Muell...   \n",
       "3  FBI Russia probe helped by Australian diplomat...   \n",
       "4  Trump wants Postal Service to charge 'much mor...   \n",
       "\n",
       "                                                text       subject  \\\n",
       "0  WASHINGTON (Reuters) - The head of a conservat...  politicsNews   \n",
       "1  WASHINGTON (Reuters) - Transgender people will...  politicsNews   \n",
       "2  WASHINGTON (Reuters) - The special counsel inv...  politicsNews   \n",
       "3  WASHINGTON (Reuters) - Trump campaign adviser ...  politicsNews   \n",
       "4  SEATTLE/WASHINGTON (Reuters) - President Donal...  politicsNews   \n",
       "\n",
       "                 date  \n",
       "0  December 31, 2017   \n",
       "1  December 29, 2017   \n",
       "2  December 31, 2017   \n",
       "3  December 30, 2017   \n",
       "4  December 29, 2017   "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"True articles\")\n",
    "true_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fake articles\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Donald Trump Sends Out Embarrassing New Year’...</td>\n",
       "      <td>Donald Trump just couldn t wish all Americans ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Drunk Bragging Trump Staffer Started Russian ...</td>\n",
       "      <td>House Intelligence Committee Chairman Devin Nu...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sheriff David Clarke Becomes An Internet Joke...</td>\n",
       "      <td>On Friday, it was revealed that former Milwauk...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 30, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trump Is So Obsessed He Even Has Obama’s Name...</td>\n",
       "      <td>On Christmas day, Donald Trump announced that ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 29, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pope Francis Just Called Out Donald Trump Dur...</td>\n",
       "      <td>Pope Francis used his annual Christmas Day mes...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 25, 2017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0   Donald Trump Sends Out Embarrassing New Year’...   \n",
       "1   Drunk Bragging Trump Staffer Started Russian ...   \n",
       "2   Sheriff David Clarke Becomes An Internet Joke...   \n",
       "3   Trump Is So Obsessed He Even Has Obama’s Name...   \n",
       "4   Pope Francis Just Called Out Donald Trump Dur...   \n",
       "\n",
       "                                                text subject  \\\n",
       "0  Donald Trump just couldn t wish all Americans ...    News   \n",
       "1  House Intelligence Committee Chairman Devin Nu...    News   \n",
       "2  On Friday, it was revealed that former Milwauk...    News   \n",
       "3  On Christmas day, Donald Trump announced that ...    News   \n",
       "4  Pope Francis used his annual Christmas Day mes...    News   \n",
       "\n",
       "                date  \n",
       "0  December 31, 2017  \n",
       "1  December 31, 2017  \n",
       "2  December 30, 2017  \n",
       "3  December 29, 2017  \n",
       "4  December 25, 2017  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Fake articles\")\n",
    "fake_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>21417</td>\n",
       "      <td>21417</td>\n",
       "      <td>21417</td>\n",
       "      <td>21417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>20826</td>\n",
       "      <td>21192</td>\n",
       "      <td>2</td>\n",
       "      <td>716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Factbox: Trump fills top jobs for his administ...</td>\n",
       "      <td>(Reuters) - Highlights for U.S. President Dona...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 20, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "      <td>11272</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    title  \\\n",
       "count                                               21417   \n",
       "unique                                              20826   \n",
       "top     Factbox: Trump fills top jobs for his administ...   \n",
       "freq                                                   14   \n",
       "\n",
       "                                                     text       subject  \\\n",
       "count                                               21417         21417   \n",
       "unique                                              21192             2   \n",
       "top     (Reuters) - Highlights for U.S. President Dona...  politicsNews   \n",
       "freq                                                    8         11272   \n",
       "\n",
       "                      date  \n",
       "count                21417  \n",
       "unique                 716  \n",
       "top     December 20, 2017   \n",
       "freq                   182  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 6 duplicate title rows from true articles\n",
      "Removed 184 duplicate title rows from fake articles\n",
      "\n",
      "Removed 229 duplicate text rows from true articles\n",
      "Removed 5398 duplicate text rows from fake articles\n",
      "______________________________________________________________________\n",
      "Removed 235 duplicate rows from true articles in total\n",
      "Removed 5582 duplicate rows from fake articles in total\n"
     ]
    }
   ],
   "source": [
    "# Creating a single dataset with labels for true = 1 and false = 0\n",
    "\n",
    "true_data['label'] = 1\n",
    "fake_data['label'] = 0\n",
    "\n",
    "# Cleaning true_data before concatinating\n",
    "## Removing journal identifier\n",
    "true_data['text'] = true_data['text'].str.partition('- ')[2]\n",
    "\n",
    "# Removing duplicates rows of titles where text is empty\n",
    "# Create a placeholder to check if text is empty\n",
    "is_text_functionally_empty = (\n",
    "    true_data['text'].isna() | \n",
    "    true_data['text'].astype(str).str.strip().eq('')\n",
    ")\n",
    "rows_to_drop = true_data[is_text_functionally_empty].duplicated(subset=['title'], keep='first')\n",
    "drop_indices = true_data[is_text_functionally_empty][rows_to_drop].index\n",
    "cleaned_true = true_data.drop(index=drop_indices)\n",
    "# Doing the same for fake articles\n",
    "fake_is_text_functionally_empty = (\n",
    "    fake_data['text'].isna() | \n",
    "    fake_data['text'].astype(str).str.strip().eq('')\n",
    ")\n",
    "rows_to_drop = fake_data[fake_is_text_functionally_empty].duplicated(subset=['title'], keep='first')\n",
    "drop_indices = fake_data[fake_is_text_functionally_empty][rows_to_drop].index\n",
    "cleaned_fake = fake_data.drop(index=drop_indices)\n",
    "\n",
    "print(f\"Removed {len(true_data)-len(cleaned_true)} duplicate title rows from true articles\")\n",
    "print(f\"Removed {len(fake_data)-len(cleaned_fake)} duplicate title rows from fake articles\")\n",
    "print(\"\")\n",
    "\n",
    "# Removing duplicates of text but keeping unique rows of title\n",
    "has_content = ~(\n",
    "    cleaned_fake['text'].isna() |\n",
    "    cleaned_fake['text'].astype(str).str.strip().eq('')\n",
    ")\n",
    "rows_to_remove = cleaned_fake[has_content].duplicated(subset=['text'], keep='first')\n",
    "\n",
    "drop_indices = cleaned_fake[has_content][rows_to_remove].index\n",
    "\n",
    "new_cleaned_fake = cleaned_fake.drop(index=drop_indices)\n",
    "\n",
    "# Doing the same for true articles\n",
    "\n",
    "has_content = ~(\n",
    "    cleaned_true['text'].isna() |\n",
    "    cleaned_true['text'].astype(str).str.strip().eq('')\n",
    ")\n",
    "rows_to_remove = cleaned_true[has_content].duplicated(subset=['text'], keep='first')\n",
    "\n",
    "drop_indices = cleaned_true[has_content][rows_to_remove].index\n",
    "\n",
    "new_cleaned_true = cleaned_true.drop(index=drop_indices)\n",
    "\n",
    "print(f\"Removed {len(cleaned_true)-len(new_cleaned_true)} duplicate text rows from true articles\")\n",
    "print(f\"Removed {len(cleaned_fake)-len(new_cleaned_fake)} duplicate text rows from fake articles\")\n",
    "print(\"______________________________________________________________________\")\n",
    "print(f\"Removed {len(true_data)-len(new_cleaned_true)} duplicate rows from true articles in total\")\n",
    "print(f\"Removed {len(fake_data)-len(new_cleaned_fake)} duplicate rows from fake articles in total\")\n",
    "\n",
    "# Concatinating dataframes\n",
    "data = pd.concat([new_cleaned_true, new_cleaned_fake])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text standardizing\n",
    "import string\n",
    "\n",
    "\n",
    "# Creating new columns to preserve original text\n",
    "data['title_standard'] = data['title']\n",
    "data['text_standard'] = data['text']\n",
    "\n",
    "# Removing punctuations including special letter which didn't get picked up by string.punctuation\n",
    "punctuation_and_special = string.punctuation + '“”‘’' \n",
    "punctuation = str.maketrans('', '', punctuation_and_special)\n",
    "\n",
    "data['title_standard'] = data['title'].astype(str).str.translate(punctuation)\n",
    "data['text_standard'] = data['text'].astype(str).str.translate(punctuation)\n",
    "\n",
    "# Lowercasing \n",
    "data['title_standard'] = data['title_standard'].astype(str).str.strip().str.lower()\n",
    "data['text_standard'] = data['text_standard'].astype(str).str.strip().str.lower()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Naive Bayes Classification Results (Using Raw Word Counts) ---\n",
      "Accuracy: 0.9424\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.91      0.94      3580\n",
      "           1       0.93      0.97      0.95      4237\n",
      "\n",
      "    accuracy                           0.94      7817\n",
      "   macro avg       0.94      0.94      0.94      7817\n",
      "weighted avg       0.94      0.94      0.94      7817\n",
      "\n",
      "--- Top 20 Words Predicting FAKE News (Label 0) ---\n",
      "| feature             |   fake_score |\n",
      "|:--------------------|-------------:|\n",
      "| 2017realdonaldtrump |      7.0647  |\n",
      "| 21wire              |      6.35848 |\n",
      "| belowfeatured       |      6.28281 |\n",
      "| getty               |      6.2585  |\n",
      "| 2017the             |      6.08819 |\n",
      "| flickr              |      6.06944 |\n",
      "| 21wiretv            |      5.91799 |\n",
      "| 2016realdonaldtrump |      5.82926 |\n",
      "| somodevillagetty    |      5.74693 |\n",
      "| screenshot          |      5.74319 |\n",
      "| acr                 |      5.70112 |\n",
      "| cdata               |      5.68933 |\n",
      "| js                  |      5.66127 |\n",
      "| filessupport        |      5.63658 |\n",
      "| 2017trump           |      5.53122 |\n",
      "| wonggetty           |      5.47895 |\n",
      "| 2016the             |      5.45426 |\n",
      "| reilly              |      5.38169 |\n",
      "| finicum             |      5.35995 |\n",
      "| angerergetty        |      5.35444 |\n",
      "\n",
      "--- Top 20 Words Predicting TRUE News (Label 1) ---\n",
      "| feature    |   fake_score |\n",
      "|:-----------|-------------:|\n",
      "| rohingya   |     -7.25682 |\n",
      "| myanmar    |     -6.45431 |\n",
      "| rakhine    |     -6.43847 |\n",
      "| puigdemont |     -6.0706  |\n",
      "| partys     |     -5.9148  |\n",
      "| fdp        |     -5.88906 |\n",
      "| zuma       |     -5.7973  |\n",
      "| suu        |     -5.76295 |\n",
      "| kyi        |     -5.75486 |\n",
      "| hariri     |     -5.66858 |\n",
      "| rajoy      |     -5.61238 |\n",
      "| mnangagwa  |     -5.55449 |\n",
      "| odinga     |     -5.5445  |\n",
      "| anc        |     -5.53103 |\n",
      "| catalan    |     -5.47407 |\n",
      "| chinas     |     -5.42001 |\n",
      "| juncker    |     -5.40474 |\n",
      "| countrys   |     -5.30789 |\n",
      "| barnier    |     -5.30364 |\n",
      "| kuczynski  |     -5.28214 |\n"
     ]
    }
   ],
   "source": [
    "# Baseline naive bayes model for article texts\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "\n",
    "# Define Features (X) and Target (y)\n",
    "X = data['text_standard']  \n",
    "y = data['label']\n",
    "\n",
    "# Splitting the data and making sure we get the same number of labels in each set with stratify=y\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Setting max_df to 0.7 we exclude words that appears in more than 70% of all articles in the training set so we should get more unique words\n",
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_counts = count_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data\n",
    "X_test_counts = count_vectorizer.transform(X_test)\n",
    "\n",
    "# Train Multinomial Naive Bayes Classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(X_train_counts, y_train)\n",
    "\n",
    "# 5. Predict and Evaluate\n",
    "y_pred_counts = nb_classifier.predict(X_test_counts)\n",
    "\n",
    "# Get the feature names to see which words are helping us predict\n",
    "feature_names = count_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Getting log coefficients \n",
    "log_probs_fake = nb_classifier.feature_log_prob_[0]\n",
    "log_probs_true = nb_classifier.feature_log_prob_[1]\n",
    "\n",
    "# Create a DataFrame so we can sort them \n",
    "feature_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'log_prob_fake': log_probs_fake,\n",
    "    'log_prob_true': log_probs_true\n",
    "})\n",
    "\n",
    "# Calculate the difference between coefficients \n",
    "# A larger positive difference means the word is highly associated with fake news\n",
    "feature_df['fake_score'] = feature_df['log_prob_fake'] - feature_df['log_prob_true']\n",
    "\n",
    "# 5. Get top 20 for Fake (highest positive scores)\n",
    "top_fake_features = feature_df.sort_values(by='fake_score', ascending=False).head(20)\n",
    "\n",
    "# 6. Get top 20 for True (lowest negative scores)\n",
    "top_true_features = feature_df.sort_values(by='fake_score', ascending=True).head(20)\n",
    "\n",
    "\n",
    "# Print results\n",
    "print(\"--- Naive Bayes Classification Results (Using Raw Word Counts) ---\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_counts):.4f}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_counts))\n",
    "\n",
    "# Print important classifying words\n",
    "print(\"--- Top 20 Words Predicting FAKE News (Label 0) ---\")\n",
    "print(top_fake_features[['feature', 'fake_score']].to_markdown(index=False))\n",
    "\n",
    "print(\"\\n--- Top 20 Words Predicting TRUE News (Label 1) ---\")\n",
    "print(top_true_features[['feature', 'fake_score']].to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Naive Bayes Classification Results (Using Raw Word Counts) ---\n",
      "Accuracy: 0.9364\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.94      0.93      3580\n",
      "           1       0.95      0.93      0.94      4237\n",
      "\n",
      "    accuracy                           0.94      7817\n",
      "   macro avg       0.94      0.94      0.94      7817\n",
      "weighted avg       0.94      0.94      0.94      7817\n",
      "\n",
      "--- Top 20 Words Predicting FAKE News (Label 0) ---\n",
      "| feature     |   fake_score |\n",
      "|:------------|-------------:|\n",
      "| hillarys    |      5.25011 |\n",
      "| wow         |      5.09675 |\n",
      "| video       |      5.09181 |\n",
      "| heres       |      5.09121 |\n",
      "| hilarious   |      4.89547 |\n",
      "| busted      |      4.62564 |\n",
      "| bombshell   |      4.51338 |\n",
      "| hilariously |      4.49337 |\n",
      "| epic        |      4.47297 |\n",
      "| sarah       |      4.45213 |\n",
      "| gop         |      4.44685 |\n",
      "| supporter   |      4.43085 |\n",
      "| fck         |      4.32907 |\n",
      "| brilliant   |      4.32907 |\n",
      "| awesome     |      4.30497 |\n",
      "| dem         |      4.30497 |\n",
      "| lol         |      4.30497 |\n",
      "| disgusting  |      4.24206 |\n",
      "| racist      |      4.24206 |\n",
      "| boiler      |      4.21574 |\n",
      "\n",
      "--- Top 20 Words Predicting TRUE News (Label 1) ---\n",
      "| feature    |   fake_score |\n",
      "|:-----------|-------------:|\n",
      "| factbox    |     -5.88249 |\n",
      "| myanmar    |     -5.38495 |\n",
      "| catalan    |     -5.17065 |\n",
      "| rohingya   |     -5.13218 |\n",
      "| kurdish    |     -4.77457 |\n",
      "| envoy      |     -4.73647 |\n",
      "| xi         |     -4.6126  |\n",
      "| referendum |     -4.47119 |\n",
      "| catalonia  |     -4.45845 |\n",
      "| ria        |     -4.40581 |\n",
      "| zimbabwe   |     -4.36443 |\n",
      "| hariri     |     -4.27613 |\n",
      "| zimbabwes  |     -4.22888 |\n",
      "| asia       |     -4.22888 |\n",
      "| bangladesh |     -4.21262 |\n",
      "| ireland    |     -4.21262 |\n",
      "| kurds      |     -4.16219 |\n",
      "| chinas     |     -4.11813 |\n",
      "| mugabe     |     -4.10908 |\n",
      "| uks        |     -4.09073 |\n"
     ]
    }
   ],
   "source": [
    "# We do the same for the article titles\n",
    "\n",
    "X = data['title_standard']\n",
    "y = data['label']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "\n",
    "X_train_counts = count_vectorizer.fit_transform(X_train)\n",
    "\n",
    "\n",
    "X_test_counts = count_vectorizer.transform(X_test)\n",
    "\n",
    "\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(X_train_counts, y_train)\n",
    "\n",
    "\n",
    "y_pred_counts = nb_classifier.predict(X_test_counts)\n",
    "\n",
    "feature_names = count_vectorizer.get_feature_names_out()\n",
    "\n",
    "log_probs_fake = nb_classifier.feature_log_prob_[0]\n",
    "log_probs_true = nb_classifier.feature_log_prob_[1]\n",
    "\n",
    "\n",
    "feature_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'log_prob_fake': log_probs_fake,\n",
    "    'log_prob_true': log_probs_true\n",
    "})\n",
    "\n",
    "feature_df['fake_score'] = feature_df['log_prob_fake'] - feature_df['log_prob_true']\n",
    "top_fake_features = feature_df.sort_values(by='fake_score', ascending=False).head(20)\n",
    "top_true_features = feature_df.sort_values(by='fake_score', ascending=True).head(20)\n",
    "\n",
    "print(\"--- Naive Bayes Classification Results (Using Raw Word Counts) ---\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_counts):.4f}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_counts))\n",
    "print(\"--- Top 20 Words Predicting FAKE News (Label 0) ---\")\n",
    "print(top_fake_features[['feature', 'fake_score']].to_markdown(index=False))\n",
    "print(\"\\n--- Top 20 Words Predicting TRUE News (Label 1) ---\")\n",
    "print(top_true_features[['feature', 'fake_score']].to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Added cleaning step to handle noise in fake articles\n",
    "data['text_cleaned'] = data['text_standard']\n",
    "# Remove common URL patterns and link shorteners\n",
    "data['text_cleaned'] = data['text_cleaned'].str.replace(r'http[s]?://\\S+|www\\.\\S+|\\S+\\.(com|org|net|co|ly)|pictwittercom|httpstco|bitly', '', regex=True)\n",
    "# Remove photo/site credit words (getty, flickr, wikimedia, etc.)\n",
    "credit_patterns = r'getty|flickr|wikimedia|belowfeatured|somodevillagetty|mcnameegetty|angerergetty|wiretv|acr|cdata|filessupport'\n",
    "data['text_cleaned'] = data['text_cleaned'].str.replace(credit_patterns, '', regex=True)\n",
    "# Remove common code snippets and internal tags\n",
    "code_patterns = r'var|js|dgetelementsbytagnames|dcreateelements|dgetelementbyidid|jssrc|jsid|wfb|featured|screenshot|raedle|gage|donnell|whinedr|src|xfbml|parentnodeinsertbefore|versionv|screengrab|subscribing|nyp'\n",
    "data['text_cleaned'] = data['text_cleaned'].str.replace(code_patterns, ' ', regex=True)\n",
    "# Find all words that consist only of letters (a-z) and more than 2 characters long to get rid of fx 21Wire\n",
    "data['text_cleaned'] = data['text_cleaned'].str.findall(r'[a-z]{2,}')\n",
    "# Join the tokenized words into single string again\n",
    "data['text_cleaned'] = data['text_cleaned'].str.join(' ')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive bayes v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Naive Bayes Classification Results (Using Raw Word Counts) ---\n",
      "Accuracy: 0.9418\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.91      0.94      3580\n",
      "           1       0.93      0.96      0.95      4237\n",
      "\n",
      "    accuracy                           0.94      7817\n",
      "   macro avg       0.94      0.94      0.94      7817\n",
      "weighted avg       0.94      0.94      0.94      7817\n",
      "\n",
      "--- Top 20 Words Predicting FAKE News (Label 0) ---\n",
      "| feature        |   fake_score |\n",
      "|:---------------|-------------:|\n",
      "| reilly         |      5.38192 |\n",
      "| finicum        |      5.36565 |\n",
      "| fcking         |      5.23757 |\n",
      "| henningsen     |      5.18661 |\n",
      "| whined         |      5.09064 |\n",
      "| bundy          |      5.05214 |\n",
      "| hammonds       |      4.81013 |\n",
      "| behar          |      4.81013 |\n",
      "| fck            |      4.7909  |\n",
      "| shit           |      4.7844  |\n",
      "| somodevilla    |      4.77129 |\n",
      "| watters        |      4.75129 |\n",
      "| elizabethforma |      4.74114 |\n",
      "| cher           |      4.72052 |\n",
      "| hilariously    |      4.71005 |\n",
      "| sarahpalinusa  |      4.69946 |\n",
      "| tantrum        |      4.69413 |\n",
      "| itthe          |      4.68877 |\n",
      "| hissy          |      4.68877 |\n",
      "| gitmo          |      4.67796 |\n",
      "\n",
      "--- Top 20 Words Predicting TRUE News (Label 1) ---\n",
      "| feature    |   fake_score |\n",
      "|:-----------|-------------:|\n",
      "| rohingya   |     -7.2566  |\n",
      "| myanmar    |     -6.45408 |\n",
      "| rakhine    |     -6.43824 |\n",
      "| puigdemont |     -6.07037 |\n",
      "| partys     |     -5.91457 |\n",
      "| zuma       |     -5.79707 |\n",
      "| suu        |     -5.76273 |\n",
      "| kyi        |     -5.75463 |\n",
      "| hariri     |     -5.66836 |\n",
      "| rajoy      |     -5.61215 |\n",
      "| mnangagwa  |     -5.55426 |\n",
      "| odinga     |     -5.54427 |\n",
      "| anc        |     -5.53081 |\n",
      "| catalan    |     -5.47385 |\n",
      "| tmsnrtrs   |     -5.47145 |\n",
      "| chinas     |     -5.41978 |\n",
      "| juncker    |     -5.40451 |\n",
      "| countrys   |     -5.30766 |\n",
      "| barnier    |     -5.30342 |\n",
      "| kuczynski  |     -5.28191 |\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define Features (X) and Target (y)\n",
    "X = data['text_cleaned']  \n",
    "y = data['label']\n",
    "\n",
    "# Splitting the data and making sure we get the same number of labels in each set with stratify=y\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Setting max_df to 0.7 we exclude words that appears in more than 70% of all articles in the training set so we should get more unique words\n",
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_counts = count_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data\n",
    "X_test_counts = count_vectorizer.transform(X_test)\n",
    "\n",
    "# Train Multinomial Naive Bayes Classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(X_train_counts, y_train)\n",
    "\n",
    "# 5. Predict and Evaluate\n",
    "y_pred_counts = nb_classifier.predict(X_test_counts)\n",
    "\n",
    "# Get the feature names to see which words are helping us predict\n",
    "feature_names = count_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Getting log coefficients \n",
    "log_probs_fake = nb_classifier.feature_log_prob_[0]\n",
    "log_probs_true = nb_classifier.feature_log_prob_[1]\n",
    "\n",
    "# Create a DataFrame so we can sort them \n",
    "feature_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'log_prob_fake': log_probs_fake,\n",
    "    'log_prob_true': log_probs_true\n",
    "})\n",
    "\n",
    "# Calculate the difference between coefficients \n",
    "# A larger positive difference means the word is highly associated with fake news\n",
    "feature_df['fake_score'] = feature_df['log_prob_fake'] - feature_df['log_prob_true']\n",
    "\n",
    "# 5. Get top 20 for Fake (highest positive scores)\n",
    "top_fake_features = feature_df.sort_values(by='fake_score', ascending=False).head(20)\n",
    "\n",
    "# 6. Get top 20 for True (lowest negative scores)\n",
    "top_true_features = feature_df.sort_values(by='fake_score', ascending=True).head(20)\n",
    "\n",
    "\n",
    "# Print results\n",
    "print(\"--- Naive Bayes Classification Results (Using Raw Word Counts) ---\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_counts):.4f}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_counts))\n",
    "\n",
    "# Print important classifying words\n",
    "print(\"--- Top 20 Words Predicting FAKE News (Label 0) ---\")\n",
    "print(top_fake_features[['feature', 'fake_score']].to_markdown(index=False))\n",
    "\n",
    "print(\"\\n--- Top 20 Words Predicting TRUE News (Label 1) ---\")\n",
    "print(top_true_features[['feature', 'fake_score']].to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/NewsExam_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Batches: 100%|██████████| 1222/1222 [00:24<00:00, 50.82it/s]\n",
      "Batches: 100%|██████████| 1222/1222 [03:01<00:00,  6.75it/s]\n"
     ]
    }
   ],
   "source": [
    "# CREATING EMBEDDINGS\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Initialize the encoder\n",
    "## all-miniLM-L6-v2 truncates sentences longer than 256 words, meaning that only the first 256 words of the sentences are embedded\n",
    "sent_encoder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Extract columns and convert to lists (using existing column names)\n",
    "title_list = data['title'].tolist()\n",
    "# Comment out text since the text in the articles can be very long, and maybe the titles are all we need.\n",
    "text_list = data['text'].tolist()\n",
    "\n",
    "# Generate embeddings\n",
    "\n",
    "title_embeddings = sent_encoder.encode(title_list, show_progress_bar=True)\n",
    "text_embeddings = sent_encoder.encode(text_list, show_progress_bar=True)\n",
    "\n",
    "# Correctly store the 2D arrays back into the DataFrame\n",
    "data['title_embedding'] = pd.Series(list(title_embeddings), index=data.index)\n",
    "data['text_embedding'] = pd.Series(list(text_embeddings), index=data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(384,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['title_embedding'].iloc[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/NewsExam_env/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n",
      "OMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n"
     ]
    }
   ],
   "source": [
    "# Dimensionality reduction for better to handle large matrices\n",
    "import umap\n",
    "\n",
    "# Use the full title_embeddings matrix (UMAP can handle it better than t-SNE)\n",
    "reducer = umap.UMAP(n_components=3, random_state=20)\n",
    "X_umap = reducer.fit_transform(title_embeddings)\n",
    "Y_umap = reducer.fit_transform(text_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding dimensionality reduced embeddings to list\n",
    "data['title_dimreduced_embedding'] = pd.Series(list(X_umap), index=data.index)\n",
    "data['text_dimreduced_embedding'] = pd.Series(list(Y_umap), index=data.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying with cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = np.stack(data['title_dimreduced_embedding'].values)\n",
    "y = data['label'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame({'title_dimreduced_embedding': list(X_train), 'label': y_train})\n",
    "\n",
    "title_mean = df_train.groupby('label')['title_dimreduced_embedding'].apply(\n",
    "    lambda x: np.mean(np.stack(x.values), axis=0)\n",
    ")\n",
    "\n",
    "title_mean_true = title_mean[1]\n",
    "title_mean_fake = title_mean[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- CLASSIFICATION RESULTS (Nearest Centroid) ---\n",
      "Accuracy on UNSEEN Test Set: 0.7595\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Fake (0)       0.69      0.85      0.76      3580\n",
      "    True (1)       0.84      0.68      0.76      4237\n",
      "\n",
      "    accuracy                           0.76      7817\n",
      "   macro avg       0.77      0.77      0.76      7817\n",
      "weighted avg       0.77      0.76      0.76      7817\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "\n",
    "mean_matrix = np.stack([title_mean_fake, title_mean_true])\n",
    "\n",
    "\n",
    "distance_matrix = cosine_distances(X_test, mean_matrix)\n",
    "\n",
    "\n",
    "y_pred = np.argmin(distance_matrix, axis=1)\n",
    "\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred, target_names=['Fake (0)', 'True (1)'])\n",
    "\n",
    "print(\"--- CLASSIFICATION RESULTS (Nearest Centroid) ---\")\n",
    "print(f\"Accuracy on UNSEEN Test Set: {accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying topic classifier to 39081 articles. This might take a while...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39081/39081 [1:14:19<00:00,  8.76it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "topic_classifier = pipeline(\"text-classification\", model=\"classla/multilingual-IPTC-news-topic-classifier\", device=0, max_length=512, truncation=True)\n",
    "\n",
    "\n",
    "print(f\"Applying topic classifier to {len(data['text_cleaned'])} articles. This might take a while...\")\n",
    "data['topic_predictions'] = data['text_cleaned'].progress_apply(lambda x: topic_classifier(x)[0] if pd.notna(x) and x != '' else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2k/3k3j84pn3pz6hg3gmlkzyctr0000gn/T/ipykernel_11094/3742753578.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  testdata['placeholder'] = testdata['text'].str.split(\".\")\n",
      "/var/folders/2k/3k3j84pn3pz6hg3gmlkzyctr0000gn/T/ipykernel_11094/3742753578.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  testdata['split_text'] = testdata['placeholder'].apply(clean_sentences)\n"
     ]
    }
   ],
   "source": [
    "testdata = data[0:10]\n",
    "\n",
    "# Apply the split, which returns a list of strings for each row\n",
    "testdata['placeholder'] = testdata['text'].str.split(\".\")\n",
    "\n",
    "\n",
    "# Clean the resulting list of strings:\n",
    "def clean_sentences(sentence_list):\n",
    "    \"\"\"Strips whitespace and filters out empty strings from the list.\"\"\"\n",
    "    \n",
    "    # 1. Strip leading/trailing whitespace from every element\n",
    "    cleaned_list = [s.strip() for s in sentence_list]\n",
    "    \n",
    "    # 2. Filter out elements that are empty ('' or just whitespace) \n",
    "    # and ensure they have a minimum length (e.g., > 1 character)\n",
    "    final_sentences = [s for s in cleaned_list if len(s) > 1]\n",
    "    \n",
    "    return final_sentences\n",
    "\n",
    "# Apply the cleaning function to the new column\n",
    "testdata['split_text'] = testdata['placeholder'].apply(clean_sentences)\n",
    "\n",
    "\n",
    "\n",
    "# sent_encoder = SentenceTransformer('distilbert-base-uncased')\n",
    "# _embeddings = sent_encoder.encode(paragraphs, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'testdata' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m punctuation_and_special \u001b[38;5;241m=\u001b[39m string\u001b[38;5;241m.\u001b[39mpunctuation \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m“”‘’\u001b[39m\u001b[38;5;124m'\u001b[39m \n\u001b[1;32m      6\u001b[0m punctuation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m\u001b[38;5;241m.\u001b[39mmaketrans(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, punctuation_and_special)\n\u001b[0;32m----> 8\u001b[0m testdata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msplit_text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mtestdata\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mtranslate(punctuation)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Lowercasing \u001b[39;00m\n\u001b[1;32m     11\u001b[0m testdata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msplit_text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m testdata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msplit_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mlower()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'testdata' is not defined"
     ]
    }
   ],
   "source": [
    "# Text standardizing for tokens\n",
    "import string\n",
    "\n",
    "# Removing punctuations including special letter which didn't get picked up by string.punctuation\n",
    "punctuation_and_special = string.punctuation + '“”‘’' \n",
    "punctuation = str.maketrans('', '', punctuation_and_special)\n",
    "\n",
    "testdata['split_text'] = testdata['text'].astype(str).str.translate(punctuation)\n",
    "\n",
    "# Lowercasing \n",
    "testdata['split_text'] = testdata['split_text'].astype(str).str.strip().str.lower()\n",
    "\n",
    "\n",
    "# Remove common URL patterns and link shorteners\n",
    "testdata['split_text'] = testdata['split_text'].str.replace(r'http[s]?://\\S+|www\\.\\S+|\\S+\\.(com|org|net|co|ly)|pictwittercom|httpstco|bitly', '', regex=True)\n",
    "# Remove photo/site credit words (getty, flickr, wikimedia, etc.)\n",
    "credit_patterns = r'getty|flickr|wikimedia|belowfeatured|somodevillagetty|mcnameegetty|angerergetty|wiretv|acr|cdata|filessupport'\n",
    "testdata['split_text'] = testdata['split_text'].str.replace(credit_patterns, '', regex=True)\n",
    "# Remove common code snippets and internal tags\n",
    "code_patterns = r'var|js|dgetelementsbytagnames|dcreateelements|dgetelementbyidid|jssrc|jsid|wfb|featured|screenshot|raedle|gage|donnell|whinedr|src|xfbml|parentnodeinsertbefore|versionv|screengrab|subscribing|nyp'\n",
    "testdata['split_text'] = testdata['split_text'].str.replace(code_patterns, ' ', regex=True)\n",
    "# Find all words that consist only of letters (a-z) and more than 2 characters long to get rid of fx 21Wire\n",
    "testdata['split_text'] = testdata['split_text'].str.findall(r'[a-z]{2,}')\n",
    "# Join the tokenized words into single string again\n",
    "testdata['split_text'] = testdata['split_text'].str.join(' ')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "from collections.abc import Iterable\n",
    "# A. Initial Split (We use the original 'text' column)\n",
    "data['sentence_list'] = data['text'].str.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?|\\!)\\s', expand=False)\n",
    "# NOTE: Using a more sophisticated regex split above to better handle titles/abbreviations like 'U.S.'\n",
    "\n",
    "def clean_and_filter_sentences(sentence_list):\n",
    "    \"\"\"Strips whitespace and filters out empty/short strings.\"\"\"\n",
    "    try:\n",
    "        if pd.isna(sentence_list).all():\n",
    "            return []\n",
    "    except AttributeError:\n",
    "        # Fallback for single-element NaNs (which don't have a .all() method)\n",
    "        if pd.isna(sentence_list):\n",
    "            return []\n",
    "    \n",
    "    # --------------------------------------------------------------------------\n",
    "    \n",
    "    # 1. Force array-like objects into a Python list\n",
    "    if isinstance(sentence_list, (np.ndarray, pd.Series)):\n",
    "        sentence_list = sentence_list.tolist()\n",
    "        \n",
    "    # 2. Handle non-iterable inputs (should only be a single string if it passed NaN check)\n",
    "    if not isinstance(sentence_list, Iterable) or isinstance(sentence_list, str):\n",
    "        # Wrap single non-list items into a list (coerces to string)\n",
    "        sentence_list = [str(sentence_list)]\n",
    "    # 1. Strip leading/trailing whitespace\n",
    "    cleaned_list = [s.strip() for s in sentence_list]\n",
    "    \n",
    "    # 2. Filter out elements that are empty or too short (e.g., just 'U.')\n",
    "    final_sentences = [s for s in cleaned_list if len(s) > 5]\n",
    "    \n",
    "    return final_sentences\n",
    "\n",
    "# Apply the cleaning and filtering\n",
    "data['sentences'] = data['sentence_list'].apply(clean_and_filter_sentences)\n",
    "\n",
    "\n",
    "# --- STAGE 2: IN-SENTENCE CLEANING (Apply rules to the list content) ---\n",
    "\n",
    "# Define all cleaning patterns\n",
    "punctuation_to_remove = string.punctuation + '“”‘’' \n",
    "credit_patterns = r'getty|flickr|wikimedia|belowfeatured|somodevillagetty|mcnameegetty|angerergetty|wiretv|acr|cdata|filessupport'\n",
    "code_patterns = r'var|js|dgetelementsbytagnames|dcreateelements|dgetelementbyidid|jssrc|jsid|wfb|featured|screenshot|raedle|gage|donnell|whinedr|src|xfbml|parentnodeinsertbefore|versionv|screengrab|subscribing|nyp'\n",
    "url_patterns = r'http[s]?://\\S+|www\\.\\S+|\\S+\\.(com|org|net|co|ly)|pictwittercom|httpstco|bitly'\n",
    "\n",
    "def apply_text_cleaning(sentence_list):\n",
    "    \"\"\"Applies all cleaning rules to every string inside the list.\"\"\"\n",
    "    cleaned_sentences = []\n",
    "    \n",
    "    for sentence in sentence_list:\n",
    "        text = str(sentence)\n",
    "        \n",
    "        # Lowercasing and strip\n",
    "        text = text.strip().lower()\n",
    "\n",
    "        # Remove URLs and links\n",
    "        text = re.sub(url_patterns, '', text)\n",
    "        \n",
    "        # Remove credit/code patterns\n",
    "        text = re.sub(credit_patterns, '', text)\n",
    "        text = re.sub(code_patterns, ' ', text)\n",
    "        \n",
    "        # Remove punctuation (This should happen LAST)\n",
    "        text = text.translate(str.maketrans('', '', punctuation_to_remove))\n",
    "\n",
    "        # IMPORTANT: We DO NOT use .findall(r'[a-z]{2,}') and .join(' ') here.\n",
    "        # The Sentence Transformer needs the sentences as full strings.\n",
    "        \n",
    "        cleaned_sentences.append(text)\n",
    "        \n",
    "    return cleaned_sentences\n",
    "\n",
    "# Apply the complex cleaning function\n",
    "data['text_sentence_tokens'] = data['sentences'].apply(apply_text_cleaning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning up dataframe \n",
    "data = data.drop(columns=['subject', 'sentences', 'date', 'sentence_list'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of individual sentences to embed: 588610\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from itertools import chain\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# 1. Flatten the list of lists in the 'cleaned_sentences' column \n",
    "# into one single list of all sentences.\n",
    "all_sentences = list(chain.from_iterable(data['text_sentence_tokens'].dropna()))\n",
    "\n",
    "print(f\"Total number of individual sentences to embed: {len(all_sentences)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name distilbert-base-uncased. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting embedding process...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 18395/18395 [15:05<00:00, 20.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding complete. Final embedding shape: (588610, 768)\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Sentence Transformer model\n",
    "sent_encoder = SentenceTransformer('distilbert-base-uncased')\n",
    "\n",
    "# Run the encoding on the flattened list\n",
    "print(\"Starting embedding process...\")\n",
    "X_embeddings = sent_encoder.encode(\n",
    "    all_sentences, \n",
    "    show_progress_bar=True,\n",
    "    # Use convert_to_tensor=True if you need PyTorch tensors later, \n",
    "    # but NumPy is fine for initial processing:\n",
    "    convert_to_tensor=False \n",
    ")\n",
    "\n",
    "print(f\"Embedding complete. Final embedding shape: {X_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting aggregation of sentence embeddings...\n",
      "Aggregation complete.\n",
      "Total number of aggregated article embeddings created: 39081\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# The dimension of your embedding vectors (e.g., 768 for DistilBERT)\n",
    "embedding_dimension = X_embeddings.shape[1] \n",
    "\n",
    "mean_embeddings = []\n",
    "current_index = 0\n",
    "\n",
    "print(\"Starting aggregation of sentence embeddings...\")\n",
    "\n",
    "# Iterate through each row in your DataFrame\n",
    "for index, sentence_list in data['text_sentence_tokens'].items():\n",
    "    num_sentences = len(sentence_list)\n",
    "    \n",
    "    if num_sentences == 0:\n",
    "        # Handle rows where cleaning resulted in zero valid sentences\n",
    "        article_embedding = np.zeros(embedding_dimension)\n",
    "    else:\n",
    "        # 1. Slice: Extract the sentences belonging to the current article\n",
    "        article_sentence_embeddings = X_embeddings[current_index : current_index + num_sentences]\n",
    "        \n",
    "        # 2. Mean: Calculate the average vector across all sentences (axis=0 averages down the rows)\n",
    "        article_embedding = np.mean(article_sentence_embeddings, axis=0)\n",
    "        \n",
    "        # 3. Advance: Move the pointer to the start of the next article's embeddings\n",
    "        current_index += num_sentences\n",
    "        \n",
    "    mean_embeddings.append(article_embedding)\n",
    "\n",
    "print(\"Aggregation complete.\")\n",
    "# Verification: The total number of new embeddings should match the number of rows in your DataFrame\n",
    "print(f\"Total number of aggregated article embeddings created: {len(mean_embeddings)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final feature matrix shape for classification/clustering: (39081, 768)\n"
     ]
    }
   ],
   "source": [
    "# Convert the list of mean embeddings into a NumPy array\n",
    "X_features = np.stack(mean_embeddings)\n",
    "\n",
    "# Store the final aggregated feature vector in the DataFrame\n",
    "# Storing as a list of NumPy arrays is often best for Pandas\n",
    "data['aggregated_text_embedding'] = list(X_features)\n",
    "\n",
    "print(f\"Final feature matrix shape for classification/clustering: {X_features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving once more so i can easily load without having to run everything\n",
    "# DONT USE CSV it ruins everything\n",
    "\n",
    "\n",
    "file_path = 'data.pkl'\n",
    "\n",
    "# 3. Save the DataFrame using pickle\n",
    "data.to_pickle(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Your max_length is set to 1024, but your input_length is only 408. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=204)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'summary_text': \"Topics: 1) Steven Stevenson 2) Movies 3) Ubama's Legacy 4) Muslim Training Camps 5) Borders 6) Undercover Us Border Agent 7) Coverups and Lünes 8) Border Crisiswatch 9) Vulnerability to Terror Attacks from the Nord 10) Movie Theaters across America 11) Screaming from Their Roofs 12) Demand for Border Control 13) Credit Card Online Banking 14) Pay Pal 15) Buy Now Movies 16) Christmas and Hanukah Gifts 17) Interview with Michael Llynch 18) fox and Friends 19) Muslim Terrorists 20) Border Patrolmanship Topic changes and Locations: Sentence 1: Stevenson to Movies SentENCE 2: Movies to Ubami's Promise Sentential Cost to Muslim Education Camps Sentestence 3: Muslim Learning Camps to Borders to Undercover U. Surgeoning Borders SenTence 4: Undercover Out Out of U.S. to Border Patrol Agent Sentiment to Coverup Screams from The Dwarf to Border Critic Watch Sentent to Vegetableness to Torture Attacks From the Nord Senttence 5: Border Crisis Watch to Movie TheaterScreaming of The Nord to Movie Makers acrossAmerica Sent to Special Offer to Credit Card On-Demands Sentingence 6: Free DVD s to Buy Today Movies in Time for Christmas and Hauka Hungah Towards a Special Offer Sentance to Mocking from My Roofs Sentimence 7: Screamed from Their Backs to Demand for Government Control Sentente 8: Demand for Front Front Control to Creditcard Online Banking Sentendence 9: Credit Card online Banking to Pay Pal Capabilities Sentituations to Buy now Movies Pentence 10: Buy Now Films to Halloween and Hankah Presents Sentenence 11: Christmas andhanukaha Gifts to Interview with Chaes and Friends Sentuel Hung Ah Embracing muslimts to Gateway to america iii Movie $response $content$ Theodores of the World are a Democrat who is currently in the midst of a difficult time. It is a complex and complex task that involves a lot of jihadis and jihadists in the Middle East. In the present, he also discusses the importance of addressing the need for a better future and more secure world. In addition to this special offer, there is no shortage of terrorists and terrorists are expected to be able to make their way into the United States\"}]\n"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "text_block = 'dennis michael lynch has made some shocking and eyeopening movies that address our failure as a nation to secure our borders  they come to america iii the cost of obama s legacy is his most compelling and shocking movie to date there are some terrifying moments when dennis goes to and exposes muslim training camps in america he travels to both our southern and northern us borders to show just how easily they can be crossed by anyone who cares to cross over into the united states his interviews with an undercover us border agent is shocking and exposes the coverups and lies we are being fed by our government about our border crisiswatch this shocking trailer as it shows how vulnerable our nation is to terror attacks from the norththey come to america iii is frightening and should be showing in movie theaters across america if every americans saw this movie they would be screaming from their rooftops and demanding that congress get our borders under controlif you buy one they come to america iii dvd at 1999 he will give you 2 free dvd s to share for a total of 3 moviesorder now while this special offer is still availablethey come to america iii the cost of obama s legacy price 1999 shipping 395 order comes with two additional copies for sharing 3 total in orderyou can pay by credit card online banking or by using pay pal simply by clicking the buy now button below    order your movies now to get your dvd s in time for christmas and hanukah gifts you ll want to share this movie with all of your relatives and friendshere is a very interesting interview with dennis michael lynch on fox and friends as they discuss how dennis exposes how easily muslim terrorists can make their way into our nation in his  they come to america iii  movie.'\n",
    "pipe = pipeline(\"summarization\", model=\"Falconsai/topic_change_point\")\n",
    "res1 = pipe(text_block, max_length=1024, min_length=512, do_sample=False)\n",
    "print(res1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing mean of title, text and dimensionality reduced embeddings.\n",
    "\n",
    "mean_title = []\n",
    "for i in X_train['title_dimreduced_embedding']:\n",
    "    m = np.mean(i, axis=0)\n",
    "    mean_title.append(m)\n",
    "\n",
    "mean_text = []\n",
    "for i in X_train['text_dimreduced_embedding']:\n",
    "    m = np.mean(i, axis=0)\n",
    "    mean_text.append(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding back to original dataframe\n",
    "# data['mean_title_embedding'] = pd.Series(list(mean_title), index=data.index)\n",
    "# data['mean_text_embedding'] = pd.Series(list(mean_text), index=data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class_means_title = data.groupby('label')['mean_title_embedding'].apply(lambda x: np.mean(np.stack(x.values), axis=0))\n",
    "class_means_text = data.groupby('label')['mean_text_embedding'].apply(lambda x: np.mean(np.stack(x.values), axis=0))\n",
    "\n",
    "\n",
    "mean_true_title = class_means_title[1]\n",
    "mean_fake_title = class_means_title[0]\n",
    "\n",
    "mean_true_text = class_means_text[1]\n",
    "mean_fake_text = class_means_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.25369\n",
      "6.559492\n"
     ]
    }
   ],
   "source": [
    "print(mean_true_text)\n",
    "print(mean_fake_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.805622\n",
      "2.3256469\n"
     ]
    }
   ],
   "source": [
    "print(mean_true_title)\n",
    "print(mean_fake_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating new columns to contain tokenized words \n",
    "data['title_words'] = data['title_standard'].str.split()\n",
    "data['text_words'] = data['text_standard'].str.split()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating sentence tokens\n",
    "# Removing stopwords from token columns\n",
    "\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "\n",
    "# Function to remove stopwords\n",
    "def remove_stopwords(tokenized_list):\n",
    "    text_without_stopwords = [word for word in tokenized_list if word not in stopwords]\n",
    "    return text_without_stopwords\n",
    "\n",
    "\n",
    "data['title_token'] = data['title_words'].apply(lambda x: remove_stopwords(x))\n",
    "data['text_token'] = data['text_words'].apply(lambda x: remove_stopwords(x))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
